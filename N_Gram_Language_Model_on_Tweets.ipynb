{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHNYJPXL2BoA"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO-HvY4uu66u",
        "outputId": "ef366417-c915-421c-90f1-7dbce3b40387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tweepy in c:\\users\\8psco\\anaconda3\\lib\\site-packages (4.5.0)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.0.0 in c:\\users\\8psco\\anaconda3\\lib\\site-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\8psco\\anaconda3\\lib\\site-packages (from tweepy) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\8psco\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\8psco\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\8psco\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\8psco\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\8psco\\anaconda3\\lib\\site-packages (from requests-oauthlib<2,>=1.0.0->tweepy) (3.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tweepy --upgrade\n",
        "\n",
        "import tweepy\n",
        "import random\n",
        "import configparser\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuNWPe8iXlKb"
      },
      "source": [
        "# Download and Clean Tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load tweepy client and read api keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eSw0aNcH-IiV"
      },
      "outputs": [],
      "source": [
        "config = configparser.ConfigParser()\n",
        "config.read('config.ini')\n",
        "tokens = config['Tokens']\n",
        "client = tweepy.Client(\n",
        "  bearer_token=tokens['BearerToken'],\n",
        "  consumer_key=tokens['ConsumerKey'],\n",
        "  consumer_secret=tokens['ConsumerSecret'],\n",
        "  access_token=tokens['AccessToken'],\n",
        "  access_token_secret=tokens['AccessTokenSecret'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download tweets of followed users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwelqwIAD38x",
        "outputId": "ffa8191d-60eb-4d9d-8282-aac7beab0abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving @scottpj3's tweets (360/360)..................\r"
          ]
        }
      ],
      "source": [
        "tweets = []\n",
        "\n",
        "# get user ids of followed twitter accounts\n",
        "me_id = client.get_users(usernames='pauljscottiv').data[0]['id']\n",
        "following = client.get_users_following(me_id, max_results=1000).data\n",
        "following = [(user['username'], user['id']) for user in following]\n",
        "\n",
        "# get tweets for each user id\n",
        "for i, (username, user_id) in enumerate(following):\n",
        "  print(f'Retrieving @{username}\\'s tweets ({i+1}/{len(following)})........', end='\\r')\n",
        "  users_tweets = []\n",
        "  until_id = None\n",
        "  repeat_count = 0\n",
        "  while repeat_count < 50:\n",
        "    try:\n",
        "      users_tweets = client.get_users_tweets(\n",
        "        user_id,\n",
        "        exclude=['retweets', 'replies'], \n",
        "        max_results=100,\n",
        "        until_id=until_id,\n",
        "      )\n",
        "      if users_tweets[0]:\n",
        "        until_id = users_tweets[0][-1]['id']\n",
        "        tweets += list(map(lambda x: x['text'], users_tweets[0]))\n",
        "        repeat_count += 1\n",
        "      else:\n",
        "        break\n",
        "    except:\n",
        "      print(f'TooManyRequests Exception: Waiting for cooldown to end ({i+1}/{len(following)})...', end='\\r')\n",
        "      time.sleep(60)\n",
        "\n",
        "print(f'\\nTotal tweets retrieved: {len(tweets)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print subset of tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Educators, I'm launching in less than 10 days. Start the countdown with this #TeachableMoment. Have students do some of the math my team uses to launch me to Mars or to look for seismic waves. These fun lessons might just inspire the next rocket scientist: https://t.co/tckiZTuLKM https://t.co/cs771vNoZ5\n",
            "\n",
            "i just tested positive for sitting around\n",
            "\n",
            "call my girl pulp fiction the way ion know what’s going on but i guess there’s a gimp suit\n",
            "\n",
            "this banger https://t.co/FzlCPGDg9m https://t.co/YeZbvN9aYV\n",
            "\n",
            "I’m praying for our health care workers who are on the frontlines fighting the recent surge in COVID-19 cases across Georgia. \n",
            "\n",
            "We owe it to them to do our part by getting vaccinated and wearing masks so we can finally get this virus under control. \n",
            "https://t.co/eleUEZtbQH\n",
            "\n",
            "ok https://t.co/CAAQGbJ1hE\n",
            "\n",
            "It was an honor to collaborate with Cookie Monster, one of our nation’s finest journalists: http://t.co/yooR2P11aE\n",
            "\n",
            "Taking Pets Like No Problem. https://t.co/UHzqS77o7K\n",
            "\n",
            "BREAKING: Mass arrests today at the Bureau of Indian Affairs in D.C. during the first occupation of the building in 50 yrs.  Biden Admin has utterly failed to protect ancestral lands from the fossil fuel industry.\n",
            "\n",
            "Solidarity with all frontline Defenders.\n",
            "https://t.co/dIilfnZNVX https://t.co/M3T8l4rBYh\n",
            "\n",
            "This week’s #WarioWareGetItTogether Wario Cup: Super Mario World Marathon\n",
            "\n",
            "How fast can you clear these classic Super Mario World courses? Play now and share a screenshot of your results in the replies! https://t.co/edKnsYvAZL\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def print_tweets(tweets, indices):\n",
        "    for tweet in np.array(tweets)[indices]:\n",
        "        if len(tweet) > 0:\n",
        "            print(tweet)\n",
        "            print()\n",
        "\n",
        "random_indices = list(np.random.choice(len(tweets), size=10, replace=False))\n",
        "\n",
        "print_tweets(tweets, random_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clean up tweets for n-gram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3cOCbicT8HM",
        "outputId": "be29b653-2fa5-40cf-ebcf-5f16084e6d13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "educators , i'm launching in less than 10 days . start the countdown with this #teachablemoment . have students do some of the math my team uses to launch me to mars or to look for seismic waves . these fun lessons might just inspire the next rocket scientist : \n",
            "\n",
            "i just tested positive for sitting around\n",
            "\n",
            "call my girl pulp fiction the way ion know what's going on but i guess there's a gimp suit\n",
            "\n",
            "this banger\n",
            "\n",
            "i'm praying for our health care workers who are on the frontlines fighting the recent surge in covid-19 cases across georgia . we owe it to them to do our part by getting vaccinated and wearing masks so we can finally get this virus under control . \n",
            "\n",
            "ok\n",
            "\n",
            "it was an honor to collaborate with cookie monster , one of our nation's finest journalists : http : //t . co/yoor2p11ae\n",
            "\n",
            "taking pets like no problem . \n",
            "\n",
            "breaking : mass arrests today at the bureau of indian affairs in d . c . during the first occupation of the building in 50 yrs . biden admin has utterly failed to protect ancestral lands from the fossil fuel industry . solidarity with all frontline defenders . \n",
            "\n",
            "this week's #wariowaregetittogether wario cup : super mario world marathon how fast can you clear these classic super mario world courses ? play now and share a screenshot of your results in the replies ! \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def clean_tweet(tweet):\n",
        "  tweet = remove_links(tweet)\n",
        "  tweet = re.sub(r'[\\{\\}\\[\\]\\(\\)\"“”]', '', tweet)\n",
        "  tweet = tweet.replace('’', \"'\").replace('…', '...').replace('&amp;', '&')\n",
        "  tweet = tweet.lower()\n",
        "  tweet = re.sub(r'([^\\w\\s@#_/\\'-])', r' \\1 ', tweet)\n",
        "  tweet = re.sub(r'\\s+', ' ', tweet)\n",
        "  return tweet\n",
        "\n",
        "def remove_links(tweet):\n",
        "  split_tweet = tweet.split()\n",
        "  split_tweet = [split_str for split_str in split_tweet if split_str if 'https://' not in split_str]\n",
        "  tweet = ' '.join(split_tweet)\n",
        "  return tweet\n",
        "\n",
        "cleaned_tweets = [clean_tweet(tweet) for tweet in tweets]\n",
        "np.save('tweets.npy', cleaned_tweets)\n",
        "\n",
        "print_tweets(cleaned_tweets, random_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI02-U5C_Nvr"
      },
      "source": [
        "# Create N-gram Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$P(token_i \\mid token_{i-n+1:i-1})=\\dfrac{Count(token_{i-n+1:i})}{Count(token_{i-n+1:i-1})}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VfKp6MoCbyeS"
      },
      "outputs": [],
      "source": [
        "START = '<SOT>'\n",
        "STOP = '<EOT>'\n",
        "\n",
        "class TweetModel:\n",
        "  def __init__(self, tweets, n):\n",
        "    self.n = n\n",
        "    self.counts = Counter()\n",
        "    self.context_counts = Counter()\n",
        "    self.process_tweets(tweets)\n",
        "\n",
        "  def process_tweets(self, tweets):\n",
        "    tokenized_tweets = [tweet.split() for tweet in tweets]\n",
        "    for tweet in tokenized_tweets:\n",
        "      if len(tweet) > 0:\n",
        "        tweet_ngrams = self.create_ngrams(tweet)\n",
        "        for ngram in tweet_ngrams:\n",
        "          self.counts[ngram] += 1\n",
        "    for (context, _), count in self.counts.items():\n",
        "      self.context_counts[context] += count\n",
        "\n",
        "  def random_tweet(self):\n",
        "    start_context = [START] * (self.n - 1)\n",
        "    curr_context = start_context\n",
        "    tweet = []\n",
        "    while True:\n",
        "      token = self.random_token(tuple(curr_context))\n",
        "      if token == STOP:\n",
        "        break\n",
        "      if len(' '.join(tweet)) > 280:\n",
        "        curr_context = start_context\n",
        "      tweet.append(token)\n",
        "      curr_context.pop(0)\n",
        "      curr_context.append(token)\n",
        "    tweet = ' '.join(tweet) + ' '\n",
        "    tweet = re.sub(r'(\\S) ([\\?!:;,\\.])', r'\\1\\2', tweet)\n",
        "    tweet = re.sub(r'(\\S) ([\\?!:;,\\.])', r'\\1\\2', tweet)\n",
        "    return tweet.strip()\n",
        "\n",
        "  def random_token(self, context):\n",
        "    tokens = [token for (context1, token), _ in self.counts.items() if context == context1]\n",
        "    rand = random.random()\n",
        "    random_token = None\n",
        "    total = 0\n",
        "    for token in tokens:\n",
        "      random_token = token\n",
        "      total += self.counts[(context, token)] / self.context_counts[context]\n",
        "      if total > rand:\n",
        "        break\n",
        "    return random_token\n",
        "\n",
        "  def create_ngrams(self, tokens):\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens) + 1):\n",
        "        context = []\n",
        "        for j in range(self.n - 1, 0, -1):\n",
        "            if i - j < 0:\n",
        "                context.append(START)\n",
        "            else:\n",
        "                context.append(tokens[i - j])\n",
        "        if i == len(tokens):\n",
        "            ngrams.append((tuple(context), STOP))\n",
        "        else:\n",
        "            ngrams.append((tuple(context), tokens[i]))\n",
        "    return tuple(ngrams)\n",
        "\n",
        "  def print_counts(self, n):\n",
        "    print(f'TOTAL UNIQUE NGRAMS: {len(self.counts)}')\n",
        "    print(f'TOP {n} NGRAMS:')\n",
        "    counts = self.counts.items()\n",
        "    counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
        "    for count in counts[:n]:\n",
        "      print(count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trigram model:\n",
        "\n",
        "$P(token_i \\mid token_{i-2:i-1})=\\dfrac{Count(token_{i-2:i})}{Count(token_{i-2:i-1})}$\n",
        "\n",
        "e.g. $P(\\text{is}\\mid\\text{my name})=\\dfrac{Count(\\text{my name is})}{Count(\\text{my name})}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WVU1yBo1mK-a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TOTAL UNIQUE NGRAMS: 2039273\n",
            "TOP 20 NGRAMS:\n",
            "((('.', '.'), '.'), 10856)\n",
            "((('<SOT>', '<SOT>'), 'i'), 9433)\n",
            "((('<SOT>', '<SOT>'), 'the'), 5896)\n",
            "((('!', '!'), '!'), 5651)\n",
            "((('▓', '▓'), '▓'), 3980)\n",
            "((('<SOT>', '<SOT>'), 'this'), 3849)\n",
            "((('░', '░'), '░'), 3774)\n",
            "((('http', ':'), '//t'), 2997)\n",
            "(((':', '//t'), '.'), 2997)\n",
            "((('.', '.'), '<EOT>'), 2375)\n",
            "((('<SOT>', '<SOT>'), 'if'), 2293)\n",
            "((('<SOT>', '<SOT>'), \"i'm\"), 2198)\n",
            "((('<SOT>', '<SOT>'), 'my'), 1967)\n",
            "((('<SOT>', '<SOT>'), 'a'), 1870)\n",
            "((('!', '!'), '<EOT>'), 1857)\n",
            "((('<SOT>', '<SOT>'), 'we'), 1724)\n",
            "((('<SOT>', '<SOT>'), \"it's\"), 1695)\n",
            "((('<SOT>', '<SOT>'), 'new'), 1598)\n",
            "((('<SOT>', 'this'), 'is'), 1521)\n",
            "((('<SOT>', '<SOT>'), 'just'), 1363)\n"
          ]
        }
      ],
      "source": [
        "tweets = np.load('tweets.npy')\n",
        "\n",
        "model = TweetModel(tweets=tweets, n=3)\n",
        "model.print_counts(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Q7Ipoz9ZnO"
      },
      "source": [
        "# Generate tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "m7S_4EiWfdzU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| how do i lack patience and understanding. e awards! one is |\n",
            "| asking why my daddy. their bias is hilarious when i cook   |\n",
            "| dinners for her bathroom renovation. it no matter what     |\n",
            "| please understand i had so many times before but i'll tell |\n",
            "| you how to haunt them                                      |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| from: mina san martín, chiurucu, huallanca, province       |\n",
            "| bolognesi, region ancash, peru. credit: golden hour        |\n",
            "| minerals #minerals                                         |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| it's about to buy 45 minutes and it's amazing to share     |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| whenever this starts to get it right this very much alive  |\n",
            "| at showtime.                                               |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| this pic of this beautiful wednesday. they've installed a  |\n",
            "| dog please ensure this tragedy does not exist              |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| not on same team to the penn campus, the misery.           |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| i want to grind out digital pennies                        |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| a brief history of catchphrase, dude. wanna make me feel   |\n",
            "| like it 😋                                                  |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| if this bar with friends for supporting my work computer   |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| also on google earth for a sugar daddy was and for the rt  |\n",
            "| @royalacademy i could make pulp fiction...                 |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "def print_tweet(tweet):\n",
        "  print('+------------------------------------------------------------+')\n",
        "  print('| +-------+                                                  |')\n",
        "  print('| |  ! !  | N-Gram Bot @ngrambot - 12h                       |')\n",
        "  print('| | [O_O] |                                                  |')\n",
        "  print('| |  | |  |                                                  |')\n",
        "  print('| +-------+                                                  |')\n",
        "  print('|                                                            |')\n",
        "\n",
        "  for line in tweet_to_lines(tweet):\n",
        "    padding = ' ' * (59 - len(line))\n",
        "    print('| ' + line + padding + '|')\n",
        "\n",
        "  print('|                                                            |')\n",
        "  print('| <3 31.4k                                                   |')\n",
        "  print('+------------------------------------------------------------+')\n",
        "\n",
        "def tweet_to_lines(tweet):\n",
        "  lines = []\n",
        "  curr_line = ''\n",
        "  for token in tweet.split():\n",
        "    if len(curr_line) + len(token) + 1 <= 59:\n",
        "      curr_line += token + ' '\n",
        "    else:\n",
        "      lines.append(curr_line)\n",
        "      curr_line = token + ' '\n",
        "  lines.append(curr_line)\n",
        "  return lines\n",
        "\n",
        "for i in range(10):\n",
        "  print_tweet(model.random_tweet())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "N-Gram Language Model on Tweets",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
