{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHNYJPXL2BoA"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO-HvY4uu66u",
        "outputId": "ef366417-c915-421c-90f1-7dbce3b40387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tweepy in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (4.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.11.1 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (2.26.0)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.0.0 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.11.1->tweepy) (3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.11.1->tweepy) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.11.1->tweepy) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.11.1->tweepy) (1.26.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib<2,>=1.0.0->tweepy) (3.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: emoji in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (1.6.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tweepy --upgrade\n",
        "%pip install emoji\n",
        "\n",
        "import tweepy\n",
        "import random\n",
        "import configparser\n",
        "import numpy as np\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuNWPe8iXlKb"
      },
      "source": [
        "# Download and Clean Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "eSw0aNcH-IiV"
      },
      "outputs": [],
      "source": [
        "config = configparser.ConfigParser()\n",
        "config.read('config.ini')\n",
        "\n",
        "tokens = config['Tokens']\n",
        "\n",
        "client = tweepy.Client(\n",
        "  bearer_token=tokens['BearerToken'],\n",
        "  consumer_key=tokens['ConsumerKey'],\n",
        "  consumer_secret=tokens['ConsumerSecret'],\n",
        "  access_token=tokens['AccessToken'],\n",
        "  access_token_secret=tokens['AccessTokenSecret'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwelqwIAD38x",
        "outputId": "ffa8191d-60eb-4d9d-8282-aac7beab0abc"
      },
      "outputs": [],
      "source": [
        "tweets = []\n",
        "users = [\n",
        "  'afraidofwasps',\n",
        "  'ameliaelizalde',\n",
        "  'boss_on_here',\n",
        "  'darth_erogenous',\n",
        "  'dril',\n",
        "  'i_zzzzzz',\n",
        "  'laserboat999',\n",
        "  'len0killer',\n",
        "  'Liv_Agar',\n",
        "  'lunch_enjoyer',\n",
        "  'nibiru_TRUTH',\n",
        "  'OkButStill',\n",
        "  'oldfriend99',\n",
        "  'peterxinping',\n",
        "  'pizza_jones',\n",
        "  'RadishHarmers',\n",
        "  'rajat_suresh',\n",
        "  's4m31p4n',\n",
        "  'Senn_Spud',\n",
        "  'yesitsmyaccount',\n",
        "  'ZeroSuitCamus'\n",
        "]\n",
        "\n",
        "# get twitter ids from usernames\n",
        "user_data = client.get_users(usernames=users)\n",
        "user_ids = list(map(lambda x: x['id'], user_data[0]))\n",
        "\n",
        "# get tweets for each user id\n",
        "for user_id in user_ids:\n",
        "  users_tweets = []\n",
        "  until_id = None\n",
        "  for _ in range(50):\n",
        "    users_tweets = client.get_users_tweets(\n",
        "      user_id,\n",
        "      exclude=['retweets', 'replies'], \n",
        "      max_results=100,\n",
        "      until_id=until_id,\n",
        "    )\n",
        "    if users_tweets[0]:\n",
        "      until_id = users_tweets[0][-1]['id']\n",
        "      tweets += list(map(lambda x: x['text'], users_tweets[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(texting gf) In uber. Be home soon. Cant wait to see you (accidentally pressing dictation button) Ohhhh i want a hamburger so bad. Hot dog too. Ohh man I want a mcchicken. me too. Woww I want a burger. Yeah I want a cheeseburger too. Ohhh wow me too. I want a hot dog.With the bun\n",
            "\n",
            "in my opinion we should legalize freedom\n",
            "\n",
            "when the hand holding is so good you arrive ya man's native city ðŸ”¥ðŸ¤¤ https://t.co/P8956BHogA\n",
            "\n",
            "My parents dog is whining. what do you have to whine about. Have you ever seen a man die\n",
            "\n",
            "Whats fucked up is , some will think this is good!!! https://t.co/2samsGi56q\n",
            "\n",
            "cuomo: just wanted to remind everyone i am a muslim. i am a jew. I am black. i am gay. goodbye https://t.co/K2hnurWEwA\n",
            "\n",
            "if you remove the license plate frame that advertises your dealership youre a nasty son of a bitch &amp; you want small business owners to choke\n",
            "\n",
            "thinking about doing a tweet that goes viral on smart twitter. might spend the next couple days just contemplating shit... HARD. watch this space ðŸ§ \n",
            "\n",
            "Lots of good points and insight on both sides of this arugment https://t.co/zCYwU2Ue3S\n",
            "\n",
            "I think that gate keeping is very good and important for soceity\n",
            "\n",
            "TOTAL TWEETS: 12130\n"
          ]
        }
      ],
      "source": [
        "def print_tweets(tweets, indices):\n",
        "    for tweet in np.array(tweets)[indices]:\n",
        "        if len(tweet) > 0:\n",
        "            print(tweet)\n",
        "            print()\n",
        "\n",
        "random_indices = list(np.random.choice(len(tweets), size=10, replace=False))\n",
        "\n",
        "print_tweets(tweets, random_indices)\n",
        "print(f'TOTAL TWEETS: {len(tweets)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save('weird_tweets.npy', np.array(tweets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "tweets = list(np.load('weird_tweets.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3cOCbicT8HM",
        "outputId": "be29b653-2fa5-40cf-ebcf-5f16084e6d13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ( texting gf )  In uber .  Be home soon .  Cant wait to see you  ( accidentally pressing dictation button )  Ohhhh i want a hamburger so bad .  Hot dog too .  Ohh man I want a mcchicken .  me too .  Woww I want a burger .  Yeah I want a cheeseburger too .  Ohhh wow me too .  I want a hot dog . With the bun\n",
            "\n",
            "in my opinion we should legalize freedom\n",
            "\n",
            "when the hand holding is so good you arrive ya man ' s native city  ðŸ”¥  ðŸ¤¤ \n",
            "\n",
            "My parents dog is whining .  what do you have to whine about .  Have you ever seen a man die\n",
            "\n",
            "Whats fucked up is  ,  some will think this is good !  !  ! \n",
            "\n",
            "cuomo :  just wanted to remind everyone i am a muslim .  i am a jew .  I am black .  i am gay .  goodbye\n",
            "\n",
            "if you remove the license plate frame that advertises your dealership youre a nasty son of a bitch  & amp ;  you want small business owners to choke\n",
            "\n",
            "thinking about doing a tweet that goes viral on smart twitter .  might spend the next couple days just contemplating shit .  .  .  HARD .  watch this space  ðŸ§  \n",
            "\n",
            "Lots of good points and insight on both sides of this arugment\n",
            "\n",
            "I think that gate keeping is very good and important for soceity\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def clean_tweet(tweet):\n",
        "  tweet = remove_links(tweet)\n",
        "  tweet = separate_symbols(tweet)\n",
        "  return tweet\n",
        "\n",
        "def remove_links(tweet):\n",
        "  split_tweet = tweet.split()\n",
        "  split_tweet = [split_str for split_str in split_tweet if split_str if 'https://' not in split_str]\n",
        "  tweet = ' '.join(split_tweet)\n",
        "  return tweet\n",
        "\n",
        "def separate_symbols(tweet):\n",
        "  tweet_chars = [f' {char} ' if not char.isalnum() and char != ' ' else char for char in tweet]\n",
        "  tweet = ''.join(tweet_chars)\n",
        "  return tweet\n",
        "\n",
        "cleaned_tweets = list(map(clean_tweet, tweets))\n",
        "\n",
        "print_tweets(cleaned_tweets, random_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI02-U5C_Nvr"
      },
      "source": [
        "# Create N-gram Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$P(word_i \\mid word_{i-n+1},...,word_{i-1})=\\dfrac{Count(word_{i-n+1},...,word_{i-1},word_i)}{Count(word_{i-n+1},...,word_{i-1})}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VfKp6MoCbyeS"
      },
      "outputs": [],
      "source": [
        "START = '<s>'\n",
        "STOP = '</s>'\n",
        "\n",
        "class TweetModel:\n",
        "  def __init__(self, tweets, n):\n",
        "    self.n = n\n",
        "    self.counts = Counter()\n",
        "    self.process_tweets(tweets)\n",
        "\n",
        "  def process_tweets(self, tweets):\n",
        "    tokenized_tweets = [tweet.split() for tweet in tweets]\n",
        "    for tweet in tokenized_tweets:\n",
        "      if len(tweet) > 0:\n",
        "        tweet_ngrams = self.create_ngrams(tweet)\n",
        "        for ngram in tweet_ngrams:\n",
        "          self.counts[ngram] += 1\n",
        "\n",
        "  def random_tweet(self):\n",
        "    start_context = [START] * (self.n - 1)\n",
        "    curr_context = start_context\n",
        "    tweet = []\n",
        "    while True:\n",
        "      token = self.random_token(tuple(curr_context))\n",
        "      if token == STOP:\n",
        "        break\n",
        "      tweet.append(token)\n",
        "      curr_context.pop(0)\n",
        "      curr_context.append(token)\n",
        "    return ' '.join(tweet)\n",
        "\n",
        "  def random_token(self, context):\n",
        "    tokens = [token for (context1, token), _ in self.counts.items() if context == context1]\n",
        "    rand = random.random()\n",
        "    random_token = None\n",
        "    total = 0\n",
        "    for token in tokens:\n",
        "      random_token = token\n",
        "      total += self.get_probability(context, token)\n",
        "      if total > rand:\n",
        "        break\n",
        "    return random_token\n",
        "\n",
        "  def create_ngrams(self, tokens):\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens) + 1):\n",
        "        context = []\n",
        "        for j in range(self.n - 1, 0, -1):\n",
        "            if i - j < 0:\n",
        "                context.append(START)\n",
        "            else:\n",
        "                context.append(tokens[i - j])\n",
        "        if i == len(tokens):\n",
        "            ngrams.append((tuple(context), STOP))\n",
        "        else:\n",
        "            ngrams.append((tuple(context), tokens[i]))\n",
        "    return tuple(ngrams)\n",
        "\n",
        "  def get_probability(self, context, token):\n",
        "    denominator = 0\n",
        "    for (context1, _), count in self.counts.items():\n",
        "      if context == context1:\n",
        "        denominator += count\n",
        "    return self.counts[(context, token)] / denominator\n",
        "\n",
        "  def print_counts(self, n):\n",
        "    counts = self.counts.items()\n",
        "    counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
        "    for count in counts[:n]:\n",
        "      print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WVU1yBo1mK-a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "((('<s>', '<s>'), 'I'), 947)\n",
            "((('.', '.'), '.'), 537)\n",
            "((('!', '!'), '!'), 399)\n",
            "((('<s>', '<s>'), 'i'), 355)\n",
            "((('I', 'â€™'), 'm'), 331)\n",
            "((('<s>', '<s>'), 'The'), 252)\n",
            "((('I', \"'\"), 'm'), 249)\n",
            "((('it', 'â€™'), 's'), 213)\n",
            "((('<s>', '<s>'), '('), 208)\n",
            "((('don', 'â€™'), 't'), 199)\n",
            "((('.', '.'), '</s>'), 173)\n",
            "((('<s>', '<s>'), 'the'), 172)\n",
            "((('<s>', '<s>'), 'if'), 166)\n",
            "((('ðŸ˜‰', 'ðŸ˜‰'), 'ðŸ˜‰'), 166)\n",
            "((('don', \"'\"), 't'), 159)\n",
            "((('<s>', '<s>'), 'My'), 158)\n",
            "((('!', '!'), '</s>'), 156)\n",
            "((('it', \"'\"), 's'), 142)\n",
            "((('<s>', '<s>'), 'You'), 138)\n",
            "((('<s>', '<s>'), 'It'), 136)\n"
          ]
        }
      ],
      "source": [
        "model = TweetModel(tweets=cleaned_tweets, n=3)\n",
        "model.print_counts(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Q7Ipoz9ZnO"
      },
      "source": [
        "# Generate tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "m7S_4EiWfdzU"
      },
      "outputs": [],
      "source": [
        "def print_tweet(tweet):\n",
        "  print('+------------------------------------------------------------+')\n",
        "  print('| +------+                                                   |')\n",
        "  print('| |  /\\  | N-Gram Bot @ngrambot - 12h                        |')\n",
        "  print('| | [OO] |                                                   |')\n",
        "  print('| |  []  |                                                   |')\n",
        "  print('| +------+                                                   |')\n",
        "  print('|                                                            |')\n",
        "\n",
        "  for line in tweet_to_lines(tweet):\n",
        "    padding = ' ' * (59 - len(line))\n",
        "    print('| ' + line + padding + '|')\n",
        "\n",
        "  print('|                                                            |')\n",
        "  print('| <3 31.4k                                                   |')\n",
        "  print('+------------------------------------------------------------+')\n",
        "\n",
        "def tweet_to_lines(tweet):\n",
        "  lines = []\n",
        "  curr_line = ''\n",
        "  for token in tweet.split():\n",
        "    if len(curr_line) + len(token) + 1 <= 59:\n",
        "      curr_line += token + ' '\n",
        "    else:\n",
        "      lines.append(curr_line)\n",
        "      curr_line = token + ' '\n",
        "  lines.append(curr_line)\n",
        "  return lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv4tSVyO9fL8",
        "outputId": "133bc327-151c-4da8-e9c0-3480e3982358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------+\n",
            "| +------+                                                   |\n",
            "| |  /\\  | N-Gram Bot @ngrambot - 12h                        |\n",
            "| | [OO] |                                                   |\n",
            "| |  []  |                                                   |\n",
            "| +------+                                                   |\n",
            "|                                                            |\n",
            "| My parents are crying with joy and relief . We have Water  |\n",
            "| Monsters . Stop crying . You wake up I am shot in the      |\n",
            "| garage . calm down lol yes , and then go home              |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +------+                                                   |\n",
            "| |  /\\  | N-Gram Bot @ngrambot - 12h                        |\n",
            "| | [OO] |                                                   |\n",
            "| |  []  |                                                   |\n",
            "| +------+                                                   |\n",
            "|                                                            |\n",
            "| if anyone is feeling unsafe ðŸ™                              |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +------+                                                   |\n",
            "| |  /\\  | N-Gram Bot @ngrambot - 12h                        |\n",
            "| | [OO] |                                                   |\n",
            "| |  []  |                                                   |\n",
            "| +------+                                                   |\n",
            "|                                                            |\n",
            "| Was this the pose he was shrouded in fog and wearing a     |\n",
            "| suit ) ( pausing ) I ' m going to see Jake in some kind of |\n",
            "| bored )                                                    |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +------+                                                   |\n",
            "| |  /\\  | N-Gram Bot @ngrambot - 12h                        |\n",
            "| | [OO] |                                                   |\n",
            "| |  []  |                                                   |\n",
            "| +------+                                                   |\n",
            "|                                                            |\n",
            "| insane when a rich sock puppet think about a scratch .     |\n",
            "| which is why aliens are always a moment to think of it     |\n",
            "| though                                                     |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +------+                                                   |\n",
            "| |  /\\  | N-Gram Bot @ngrambot - 12h                        |\n",
            "| | [OO] |                                                   |\n",
            "| |  []  |                                                   |\n",
            "| +------+                                                   |\n",
            "|                                                            |\n",
            "| Turns out he ' d have blond hair now                       |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +------+                                                   |\n",
            "| |  /\\  | N-Gram Bot @ngrambot - 12h                        |\n",
            "| | [OO] |                                                   |\n",
            "| |  []  |                                                   |\n",
            "| +------+                                                   |\n",
            "|                                                            |\n",
            "| my wife asks . Don â€™ t get enough of this rogan : bro good |\n",
            "| shit taking your shirt without an ice cold can of Sprite ) |\n",
            "| Dad bods are back                                          |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +------+                                                   |\n",
            "| |  /\\  | N-Gram Bot @ngrambot - 12h                        |\n",
            "| | [OO] |                                                   |\n",
            "| |  []  |                                                   |\n",
            "| +------+                                                   |\n",
            "|                                                            |\n",
            "| i will never kill spiders I find myself trapped in chinese |\n",
            "| finger traps so 5 total sets of Lateral Raises and         |\n",
            "| instantly ragdolls 30 feet into the toilet , oh man how    |\n",
            "| the predator from the matrix : when you ' re still only    |\n",
            "| Series A funded . Crazy to think it ' s a little machete   |\n",
            "| fightin â€™                                                  |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  print_tweet(model.random_tweet())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "N-Gram Language Model on Tweets",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
