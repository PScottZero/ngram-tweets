{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHNYJPXL2BoA"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO-HvY4uu66u",
        "outputId": "ef366417-c915-421c-90f1-7dbce3b40387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tweepy in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (4.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.11.1 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (2.26.0)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.0.0 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.11.1->tweepy) (3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.11.1->tweepy) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.11.1->tweepy) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.11.1->tweepy) (1.26.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib<2,>=1.0.0->tweepy) (3.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: emoji in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (1.6.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tweepy --upgrade\n",
        "%pip install emoji\n",
        "\n",
        "import tweepy\n",
        "import random\n",
        "import configparser\n",
        "import numpy as np\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuNWPe8iXlKb"
      },
      "source": [
        "# Download and Clean Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "eSw0aNcH-IiV"
      },
      "outputs": [],
      "source": [
        "config = configparser.ConfigParser()\n",
        "config.read('config.ini')\n",
        "\n",
        "tokens = config['Tokens']\n",
        "\n",
        "client = tweepy.Client(\n",
        "  bearer_token=tokens['BearerToken'],\n",
        "  consumer_key=tokens['ConsumerKey'],\n",
        "  consumer_secret=tokens['ConsumerSecret'],\n",
        "  access_token=tokens['AccessToken'],\n",
        "  access_token_secret=tokens['AccessTokenSecret'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwelqwIAD38x",
        "outputId": "ffa8191d-60eb-4d9d-8282-aac7beab0abc"
      },
      "outputs": [],
      "source": [
        "tweets = []\n",
        "users = [\n",
        "  'afraidofwasps',\n",
        "  'ameliaelizalde',\n",
        "  'boss_on_here',\n",
        "  'darth_erogenous',\n",
        "  'dril',\n",
        "  'i_zzzzzz',\n",
        "  'laserboat999',\n",
        "  'len0killer',\n",
        "  'Liv_Agar',\n",
        "  'lunch_enjoyer',\n",
        "  'nibiru_TRUTH',\n",
        "  'OkButStill',\n",
        "  'oldfriend99',\n",
        "  'peterxinping',\n",
        "  'pizza_jones',\n",
        "  'RadishHarmers',\n",
        "  'rajat_suresh',\n",
        "  's4m31p4n',\n",
        "  'Senn_Spud',\n",
        "  'yesitsmyaccount',\n",
        "  'ZeroSuitCamus'\n",
        "]\n",
        "\n",
        "# get twitter ids from usernames\n",
        "user_data = client.get_users(usernames=users)\n",
        "user_ids = list(map(lambda x: x['id'], user_data[0]))\n",
        "\n",
        "# get tweets for each user id\n",
        "for user_id in user_ids:\n",
        "  users_tweets = []\n",
        "  until_id = None\n",
        "  for _ in range(50):\n",
        "    users_tweets = client.get_users_tweets(\n",
        "      user_id,\n",
        "      exclude=['retweets', 'replies'], \n",
        "      max_results=100,\n",
        "      until_id=until_id,\n",
        "    )\n",
        "    if users_tweets[0]:\n",
        "      until_id = users_tweets[0][-1]['id']\n",
        "      tweets += list(map(lambda x: x['text'], users_tweets[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(texting gf) In uber. Be home soon. Cant wait to see you (accidentally pressing dictation button) Ohhhh i want a hamburger so bad. Hot dog too. Ohh man I want a mcchicken. me too. Woww I want a burger. Yeah I want a cheeseburger too. Ohhh wow me too. I want a hot dog.With the bun\n",
            "\n",
            "in my opinion we should legalize freedom\n",
            "\n",
            "when the hand holding is so good you arrive ya man's native city 🔥🤤 https://t.co/P8956BHogA\n",
            "\n",
            "My parents dog is whining. what do you have to whine about. Have you ever seen a man die\n",
            "\n",
            "Whats fucked up is , some will think this is good!!! https://t.co/2samsGi56q\n",
            "\n",
            "cuomo: just wanted to remind everyone i am a muslim. i am a jew. I am black. i am gay. goodbye https://t.co/K2hnurWEwA\n",
            "\n",
            "if you remove the license plate frame that advertises your dealership youre a nasty son of a bitch &amp; you want small business owners to choke\n",
            "\n",
            "thinking about doing a tweet that goes viral on smart twitter. might spend the next couple days just contemplating shit... HARD. watch this space 🧠\n",
            "\n",
            "Lots of good points and insight on both sides of this arugment https://t.co/zCYwU2Ue3S\n",
            "\n",
            "I think that gate keeping is very good and important for soceity\n",
            "\n",
            "TOTAL TWEETS: 12130\n"
          ]
        }
      ],
      "source": [
        "def print_tweets(tweets, indices):\n",
        "    for tweet in np.array(tweets)[indices]:\n",
        "        if len(tweet) > 0:\n",
        "            print(tweet)\n",
        "            print()\n",
        "\n",
        "random_indices = list(np.random.choice(len(tweets), size=10, replace=False))\n",
        "\n",
        "print_tweets(tweets, random_indices)\n",
        "print(f'TOTAL TWEETS: {len(tweets)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save('weird_tweets.npy', np.array(tweets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "tweets = list(np.load('weird_tweets.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3cOCbicT8HM",
        "outputId": "be29b653-2fa5-40cf-ebcf-5f16084e6d13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ( texting gf )  In uber .  Be home soon .  Cant wait to see you  ( accidentally pressing dictation button )  Ohhhh i want a hamburger so bad .  Hot dog too .  Ohh man I want a mcchicken .  me too .  Woww I want a burger .  Yeah I want a cheeseburger too .  Ohhh wow me too .  I want a hot dog . With the bun\n",
            "\n",
            "in my opinion we should legalize freedom\n",
            "\n",
            "when the hand holding is so good you arrive ya man ' s native city  🔥  🤤 \n",
            "\n",
            "My parents dog is whining .  what do you have to whine about .  Have you ever seen a man die\n",
            "\n",
            "Whats fucked up is  ,  some will think this is good !  !  ! \n",
            "\n",
            "cuomo :  just wanted to remind everyone i am a muslim .  i am a jew .  I am black .  i am gay .  goodbye\n",
            "\n",
            "if you remove the license plate frame that advertises your dealership youre a nasty son of a bitch  & amp ;  you want small business owners to choke\n",
            "\n",
            "thinking about doing a tweet that goes viral on smart twitter .  might spend the next couple days just contemplating shit .  .  .  HARD .  watch this space  🧠 \n",
            "\n",
            "Lots of good points and insight on both sides of this arugment\n",
            "\n",
            "I think that gate keeping is very good and important for soceity\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def clean_tweet(tweet):\n",
        "  tweet = remove_links(tweet)\n",
        "  tweet = separate_symbols(tweet)\n",
        "  return tweet\n",
        "\n",
        "def remove_links(tweet):\n",
        "  split_tweet = tweet.split()\n",
        "  split_tweet = [split_str for split_str in split_tweet if split_str if 'https://' not in split_str]\n",
        "  tweet = ' '.join(split_tweet)\n",
        "  return tweet\n",
        "\n",
        "def separate_symbols(tweet):\n",
        "  tweet_chars = [f' {char} ' if not char.isalnum() and char != ' ' else char for char in tweet]\n",
        "  tweet = ''.join(tweet_chars)\n",
        "  return tweet\n",
        "\n",
        "cleaned_tweets = list(map(clean_tweet, tweets))\n",
        "\n",
        "print_tweets(cleaned_tweets, random_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI02-U5C_Nvr"
      },
      "source": [
        "# Create N-gram Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$P(word_i \\mid word_{i-n+1},...,word_{i-1})=\\dfrac{Count(word_{i-n+1},...,word_{i-1},word_i)}{Count(word_{i-n+1},...,word_{i-1})}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VfKp6MoCbyeS"
      },
      "outputs": [],
      "source": [
        "START = '<s>'\n",
        "STOP = '</s>'\n",
        "\n",
        "class TweetModel:\n",
        "  def __init__(self, tweets, n):\n",
        "    self.n = n\n",
        "    self.counts = Counter()\n",
        "    self.process_tweets(tweets)\n",
        "\n",
        "  def process_tweets(self, tweets):\n",
        "    tokenized_tweets = [tweet.split() for tweet in tweets]\n",
        "    for tweet in tokenized_tweets:\n",
        "      if len(tweet) > 0:\n",
        "        tweet_ngrams = self.create_ngrams(tweet)\n",
        "        for ngram in tweet_ngrams:\n",
        "          self.counts[ngram] += 1\n",
        "\n",
        "  def random_tweet(self):\n",
        "    start_context = [START] * (self.n - 1)\n",
        "    curr_context = start_context\n",
        "    tweet = []\n",
        "    while True:\n",
        "      token = self.random_token(tuple(curr_context))\n",
        "      if token == STOP:\n",
        "        break\n",
        "      tweet.append(token)\n",
        "      curr_context.pop(0)\n",
        "      curr_context.append(token)\n",
        "    return ' '.join(tweet)\n",
        "\n",
        "  def random_token(self, context):\n",
        "    tokens = [token for (context1, token), _ in self.counts.items() if context == context1]\n",
        "    rand = random.random()\n",
        "    random_token = None\n",
        "    total = 0\n",
        "    for token in tokens:\n",
        "      random_token = token\n",
        "      total += self.get_probability(context, token)\n",
        "      if total > rand:\n",
        "        break\n",
        "    return random_token\n",
        "\n",
        "  def create_ngrams(self, tokens):\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens) + 1):\n",
        "        context = []\n",
        "        for j in range(self.n - 1, 0, -1):\n",
        "            if i - j < 0:\n",
        "                context.append(START)\n",
        "            else:\n",
        "                context.append(tokens[i - j])\n",
        "        if i == len(tokens):\n",
        "            ngrams.append((tuple(context), STOP))\n",
        "        else:\n",
        "            ngrams.append((tuple(context), tokens[i]))\n",
        "    return tuple(ngrams)\n",
        "\n",
        "  def get_probability(self, context, token):\n",
        "    denominator = 0\n",
        "    for (context1, _), count in self.counts.items():\n",
        "      if context == context1:\n",
        "        denominator += count\n",
        "    return self.counts[(context, token)] / denominator\n",
        "\n",
        "  def print_counts(self, n):\n",
        "    counts = self.counts.items()\n",
        "    counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
        "    for count in counts[:n]:\n",
        "      print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WVU1yBo1mK-a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "((('<s>', '<s>'), 'I'), 947)\n",
            "((('.', '.'), '.'), 537)\n",
            "((('!', '!'), '!'), 399)\n",
            "((('<s>', '<s>'), 'i'), 355)\n",
            "((('I', '’'), 'm'), 331)\n",
            "((('<s>', '<s>'), 'The'), 252)\n",
            "((('I', \"'\"), 'm'), 249)\n",
            "((('it', '’'), 's'), 213)\n",
            "((('<s>', '<s>'), '('), 208)\n",
            "((('don', '’'), 't'), 199)\n",
            "((('.', '.'), '</s>'), 173)\n",
            "((('<s>', '<s>'), 'the'), 172)\n",
            "((('<s>', '<s>'), 'if'), 166)\n",
            "((('😉', '😉'), '😉'), 166)\n",
            "((('don', \"'\"), 't'), 159)\n",
            "((('<s>', '<s>'), 'My'), 158)\n",
            "((('!', '!'), '</s>'), 156)\n",
            "((('it', \"'\"), 's'), 142)\n",
            "((('<s>', '<s>'), 'You'), 138)\n",
            "((('<s>', '<s>'), 'It'), 136)\n"
          ]
        }
      ],
      "source": [
        "model = TweetModel(tweets=cleaned_tweets, n=3)\n",
        "model.print_counts(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Q7Ipoz9ZnO"
      },
      "source": [
        "# Generate tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "m7S_4EiWfdzU"
      },
      "outputs": [],
      "source": [
        "def print_tweet(tweet):\n",
        "  print('+------------------------------------------------------------+')\n",
        "  print('| +------+                                                   |')\n",
        "  print('| |  /\\  | N-Gram Bot @ngrambot - 12h                        |')\n",
        "  print('| | [OO] |                                                   |')\n",
        "  print('| |  []  |                                                   |')\n",
        "  print('| +------+                                                   |')\n",
        "  print('|                                                            |')\n",
        "\n",
        "  for line in tweet_to_lines(tweet):\n",
        "    padding = ' ' * (59 - len(line))\n",
        "    print('| ' + line + padding + '|')\n",
        "\n",
        "  print('|                                                            |')\n",
        "  print('| <3 31.4k                                                   |')\n",
        "  print('+------------------------------------------------------------+')\n",
        "\n",
        "def tweet_to_lines(tweet):\n",
        "  lines = []\n",
        "  curr_line = ''\n",
        "  for token in tweet.split():\n",
        "    if len(curr_line) + len(token) + 1 <= 59:\n",
        "      curr_line += token + ' '\n",
        "    else:\n",
        "      lines.append(curr_line)\n",
        "      curr_line = token + ' '\n",
        "  lines.append(curr_line)\n",
        "  return lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv4tSVyO9fL8",
        "outputId": "133bc327-151c-4da8-e9c0-3480e3982358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------+\n",
            "| +------+                                                   |\n",
            "| |  /\\  | N-Gram Bot @ngrambot - 12h                        |\n",
            "| | [OO] |                                                   |\n",
            "| |  []  |                                                   |\n",
            "| +------+                                                   |\n",
            "|                                                            |\n",
            "| My parents are crying with joy and relief . We have Water  |\n",
            "| Monsters . Stop crying . You wake up I am shot in the      |\n",
            "| garage . calm down lol yes , and then go home              |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +------+                                                   |\n",
            "| |  /\\  | N-Gram Bot @ngrambot - 12h                        |\n",
            "| | [OO] |                                                   |\n",
            "| |  []  |                                                   |\n",
            "| +------+                                                   |\n",
            "|                                                            |\n",
            "| if anyone is feeling unsafe 🙏                              |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +------+                                                   |\n",
            "| |  /\\  | N-Gram Bot @ngrambot - 12h                        |\n",
            "| | [OO] |                                                   |\n",
            "| |  []  |                                                   |\n",
            "| +------+                                                   |\n",
            "|                                                            |\n",
            "| Was this the pose he was shrouded in fog and wearing a     |\n",
            "| suit ) ( pausing ) I ' m going to see Jake in some kind of |\n",
            "| bored )                                                    |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +------+                                                   |\n",
            "| |  /\\  | N-Gram Bot @ngrambot - 12h                        |\n",
            "| | [OO] |                                                   |\n",
            "| |  []  |                                                   |\n",
            "| +------+                                                   |\n",
            "|                                                            |\n",
            "| insane when a rich sock puppet think about a scratch .     |\n",
            "| which is why aliens are always a moment to think of it     |\n",
            "| though                                                     |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +------+                                                   |\n",
            "| |  /\\  | N-Gram Bot @ngrambot - 12h                        |\n",
            "| | [OO] |                                                   |\n",
            "| |  []  |                                                   |\n",
            "| +------+                                                   |\n",
            "|                                                            |\n",
            "| Turns out he ' d have blond hair now                       |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +------+                                                   |\n",
            "| |  /\\  | N-Gram Bot @ngrambot - 12h                        |\n",
            "| | [OO] |                                                   |\n",
            "| |  []  |                                                   |\n",
            "| +------+                                                   |\n",
            "|                                                            |\n",
            "| my wife asks . Don ’ t get enough of this rogan : bro good |\n",
            "| shit taking your shirt without an ice cold can of Sprite ) |\n",
            "| Dad bods are back                                          |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +------+                                                   |\n",
            "| |  /\\  | N-Gram Bot @ngrambot - 12h                        |\n",
            "| | [OO] |                                                   |\n",
            "| |  []  |                                                   |\n",
            "| +------+                                                   |\n",
            "|                                                            |\n",
            "| i will never kill spiders I find myself trapped in chinese |\n",
            "| finger traps so 5 total sets of Lateral Raises and         |\n",
            "| instantly ragdolls 30 feet into the toilet , oh man how    |\n",
            "| the predator from the matrix : when you ' re still only    |\n",
            "| Series A funded . Crazy to think it ' s a little machete   |\n",
            "| fightin ’                                                  |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  print_tweet(model.random_tweet())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "N-Gram Language Model on Tweets",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
