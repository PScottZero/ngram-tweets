{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHNYJPXL2BoA"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO-HvY4uu66u",
        "outputId": "ef366417-c915-421c-90f1-7dbce3b40387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tweepy in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (4.4.0)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.0.0 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: requests<3,>=2.11.1 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (2.26.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.11.1->tweepy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.11.1->tweepy) (3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.11.1->tweepy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.11.1->tweepy) (1.26.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Users/paulscott/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib<2,>=1.0.0->tweepy) (3.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tweepy --upgrade\n",
        "\n",
        "import tweepy\n",
        "import random\n",
        "import configparser\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuNWPe8iXlKb"
      },
      "source": [
        "# Download and Clean Tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load tweepy client and read api keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eSw0aNcH-IiV"
      },
      "outputs": [],
      "source": [
        "config = configparser.ConfigParser()\n",
        "config.read('config.ini')\n",
        "tokens = config['Tokens']\n",
        "client = tweepy.Client(\n",
        "  bearer_token=tokens['BearerToken'],\n",
        "  consumer_key=tokens['ConsumerKey'],\n",
        "  consumer_secret=tokens['ConsumerSecret'],\n",
        "  access_token=tokens['AccessToken'],\n",
        "  access_token_secret=tokens['AccessTokenSecret'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download tweets of specified users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwelqwIAD38x",
        "outputId": "ffa8191d-60eb-4d9d-8282-aac7beab0abc"
      },
      "outputs": [],
      "source": [
        "tweets = []\n",
        "users = [\n",
        "  'afraidofwasps', 'ameliaelizalde', 'Boringstein', 'boss_on_here',\n",
        "  'darth_erogenous', 'dril', 'feufillet', 'heaberald', 'i_zzzzzz',\n",
        "  'keffals', 'laserboat999', 'len0killer', 'Liv_Agar', 'lunch_enjoyer',\n",
        "  'nibiru_TRUTH', 'OkButStill', 'oldfriend99', 'peterxinping',\n",
        "  'PillsAreFood', 'pizza_jones', 'RadishHarmers', 'rajat_suresh',\n",
        "  's4m31p4n', 'Senn_Spud', 'yesitsmyaccount', 'ZeroSuitCamus',\n",
        "]\n",
        "\n",
        "# get twitter ids from usernames\n",
        "user_data = client.get_users(usernames=users)\n",
        "user_ids = list(map(lambda x: x['id'], user_data[0]))\n",
        "\n",
        "# get tweets for each user id\n",
        "for user_id in user_ids:\n",
        "  until_id = None\n",
        "  for _ in range(8):\n",
        "    users_tweets = client.get_users_tweets(\n",
        "      user_id,\n",
        "      exclude=['retweets', 'replies'], \n",
        "      max_results=100,\n",
        "      until_id=until_id,\n",
        "    )\n",
        "    if users_tweets[0]:\n",
        "      until_id = users_tweets[0][-1]['id']\n",
        "      tweets += list(map(lambda x: x['text'], users_tweets[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print subset of tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why Does White Guy Always Scream “LETS GOOOO” Like Wtf You Need To Relax Friend We’re Just Playing Checkers.\n",
            "\n",
            "Cool https://t.co/pGxNxsv92c\n",
            "\n",
            "Asking the arresting officer if it’s over\n",
            "\n",
            "The internet should be decentralized... (everyone applauds and I get more confident) It should also be 400,000 times slower and every single thing on it should be a speculative financial asset (everyone is cheering and building a throne for me)\n",
            "\n",
            "you could still make clive owen james bond. i think it would be a nice little punishment for him.\n",
            "\n",
            "I find these videos physically repulsive please stop sharing them https://t.co/66M3Gw21SN\n",
            "\n",
            "It's cool how you said that one sentence, and then said that other one that was totally unrelated. It was like you were tapping into something deeper; I couldn't understand it at all. I want my computer to be window. I want to open my laptop and look at my living room carpet\n",
            "\n",
            "April 15th, 1912. 2:20 am.\n",
            "\n",
            "She Is Drunk Telling Everyone How Happy She Is Laughing And Smiling Around Strangers Yet Texting You Has Not Even Crossed Her Mind.\n",
            "\n",
            "Be thankful, everyone. Days like Christmas don't come around every year\n",
            "\n",
            "-----\n",
            "\n",
            "TOTAL TWEETS: 14988\n"
          ]
        }
      ],
      "source": [
        "def print_tweets(tweets, indices):\n",
        "    for tweet in np.array(tweets)[indices]:\n",
        "        if len(tweet) > 0:\n",
        "            print(tweet)\n",
        "            print()\n",
        "\n",
        "random_indices = list(np.random.choice(len(tweets), size=10, replace=False))\n",
        "\n",
        "print_tweets(tweets, random_indices)\n",
        "print(f'-----\\n\\nTOTAL TWEETS: {len(tweets)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save and load tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save('weird_tweets.npy', np.array(tweets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tweets = list(np.load('weird_tweets.npy'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clean up tweets for n-gram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3cOCbicT8HM",
        "outputId": "be29b653-2fa5-40cf-ebcf-5f16084e6d13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "why does white guy always scream lets goooo like wtf you need to relax friend we're just playing checkers . \n",
            "\n",
            "cool\n",
            "\n",
            "asking the arresting officer if it's over\n",
            "\n",
            "the internet should be decentralized . . . everyone applauds and i get more confident it should also be 400 , 000 times slower and every single thing on it should be a speculative financial asset everyone is cheering and building a throne for me\n",
            "\n",
            "you could still make clive owen james bond . i think it would be a nice little punishment for him . \n",
            "\n",
            "i find these videos physically repulsive please stop sharing them\n",
            "\n",
            "it's cool how you said that one sentence , and then said that other one that was totally unrelated . it was like you were tapping into something deeper ; i couldn't understand it at all . i want my computer to be window . i want to open my laptop and look at my living room carpet\n",
            "\n",
            "april 15th , 1912 . 2 : 20 am . \n",
            "\n",
            "she is drunk telling everyone how happy she is laughing and smiling around strangers yet texting you has not even crossed her mind . \n",
            "\n",
            "be thankful , everyone . days like christmas don't come around every year\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def clean_tweet(tweet):\n",
        "  tweet = remove_links(tweet)\n",
        "  tweet = re.sub(r'[\\{\\}\\[\\]\\(\\)\"“”]', '', tweet)\n",
        "  tweet = tweet.replace('’', \"'\").replace('…', '...').replace('&amp;', '&')\n",
        "  tweet = tweet.lower()\n",
        "  tweet = re.sub(r'([^\\w\\s@#_/\\'-])', r' \\1 ', tweet)\n",
        "  tweet = re.sub(r'\\s+', ' ', tweet)\n",
        "  return tweet\n",
        "\n",
        "def remove_links(tweet):\n",
        "  split_tweet = tweet.split()\n",
        "  split_tweet = [split_str for split_str in split_tweet if split_str if 'https://' not in split_str]\n",
        "  tweet = ' '.join(split_tweet)\n",
        "  return tweet\n",
        "\n",
        "def multi_replace(text, string_map):\n",
        "  for old, new in string_map.items():\n",
        "    text = text.replace(old, new)\n",
        "  return text\n",
        "\n",
        "cleaned_tweets = list(map(clean_tweet, tweets))\n",
        "\n",
        "print_tweets(cleaned_tweets, random_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI02-U5C_Nvr"
      },
      "source": [
        "# Create N-gram Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$P(token_i \\mid token_{i-n+1:i-1})=\\dfrac{Count(token_{i-n+1:i})}{Count(token_{i-n+1:i-1})}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "VfKp6MoCbyeS"
      },
      "outputs": [],
      "source": [
        "START = '<s>'\n",
        "STOP = '</s>'\n",
        "\n",
        "class TweetModel:\n",
        "  def __init__(self, tweets, n):\n",
        "    self.n = n\n",
        "    self.counts = Counter()\n",
        "    self.context_counts = Counter()\n",
        "    self.process_tweets(tweets)\n",
        "\n",
        "  def process_tweets(self, tweets):\n",
        "    tokenized_tweets = [tweet.split() for tweet in tweets]\n",
        "    for tweet in tokenized_tweets:\n",
        "      if len(tweet) > 0:\n",
        "        tweet_ngrams = self.create_ngrams(tweet)\n",
        "        for ngram in tweet_ngrams:\n",
        "          self.counts[ngram] += 1\n",
        "    for (context, _), count in self.counts.items():\n",
        "      self.context_counts[context] += count\n",
        "\n",
        "  def random_tweet(self):\n",
        "    start_context = [START] * (self.n - 1)\n",
        "    curr_context = start_context\n",
        "    tweet = []\n",
        "    while True:\n",
        "      token = self.random_token(tuple(curr_context))\n",
        "      if token == STOP:\n",
        "        break\n",
        "      if len(' '.join(tweet)) > 280:\n",
        "        curr_context = start_context\n",
        "      tweet.append(token)\n",
        "      curr_context.pop(0)\n",
        "      curr_context.append(token)\n",
        "    tweet = ' '.join(tweet) + ' '\n",
        "    tweet = re.sub(r'(\\S) ([\\?!:;,\\.])', r'\\1\\2', tweet)\n",
        "    tweet = re.sub(r'(\\S) ([\\?!:;,\\.])', r'\\1\\2', tweet)\n",
        "    return tweet.strip()\n",
        "\n",
        "  def random_token(self, context):\n",
        "    tokens = [token for (context1, token), _ in self.counts.items() if context == context1]\n",
        "    rand = random.random()\n",
        "    random_token = None\n",
        "    total = 0\n",
        "    for token in tokens:\n",
        "      random_token = token\n",
        "      total += self.counts[(context, token)] / self.context_counts[context]\n",
        "      if total > rand:\n",
        "        break\n",
        "    return random_token\n",
        "\n",
        "  def create_ngrams(self, tokens):\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens) + 1):\n",
        "        context = []\n",
        "        for j in range(self.n - 1, 0, -1):\n",
        "            if i - j < 0:\n",
        "                context.append(START)\n",
        "            else:\n",
        "                context.append(tokens[i - j])\n",
        "        if i == len(tokens):\n",
        "            ngrams.append((tuple(context), STOP))\n",
        "        else:\n",
        "            ngrams.append((tuple(context), tokens[i]))\n",
        "    return tuple(ngrams)\n",
        "\n",
        "  def print_counts(self, n):\n",
        "    print(f'TOTAL UNIQUE NGRAMS: {len(self.counts)}')\n",
        "    print(f'TOP {n} NGRAMS:')\n",
        "    counts = self.counts.items()\n",
        "    counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
        "    for count in counts[:n]:\n",
        "      print(count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trigram model:\n",
        "\n",
        "$P(token_i \\mid token_{i-2:i-1})=\\dfrac{Count(token_{i-2:i})}{Count(token_{i-2:i-1})}$\n",
        "\n",
        "e.g. $P(\\text{is}\\mid\\text{my name})=\\dfrac{Count(\\text{my name is})}{Count(\\text{my name})}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "WVU1yBo1mK-a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TOTAL UNIQUE NGRAMS: 233066\n",
            "TOP 20 NGRAMS:\n",
            "((('<s>', '<s>'), 'i'), 1389)\n",
            "((('.', '.'), '.'), 996)\n",
            "((('<s>', '<s>'), 'the'), 536)\n",
            "((('!', '!'), '!'), 491)\n",
            "((('<s>', '<s>'), 'my'), 360)\n",
            "((('<s>', '<s>'), 'if'), 357)\n",
            "((('<s>', '<s>'), 'this'), 304)\n",
            "((('.', '.'), '</s>'), 279)\n",
            "((('<s>', '<s>'), \"i'm\"), 277)\n",
            "((('<s>', '<s>'), 'you'), 246)\n",
            "((('<s>', '<s>'), \"it's\"), 183)\n",
            "((('!', '!'), '</s>'), 179)\n",
            "((('<s>', '<s>'), 'just'), 170)\n",
            "((('😉', '😉'), '😉'), 166)\n",
            "((('<s>', '<s>'), 'when'), 157)\n",
            "((('<s>', '<s>'), 'oh'), 148)\n",
            "((('<s>', '<s>'), 'me'), 143)\n",
            "((('<s>', 'this'), 'is'), 139)\n",
            "((('<s>', '<s>'), 'they'), 133)\n",
            "((('<s>', '<s>'), 'im'), 130)\n"
          ]
        }
      ],
      "source": [
        "model = TweetModel(tweets=cleaned_tweets, n=3)\n",
        "model.print_counts(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Q7Ipoz9ZnO"
      },
      "source": [
        "# Generate tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "m7S_4EiWfdzU"
      },
      "outputs": [],
      "source": [
        "def print_tweet(tweet):\n",
        "  print('+------------------------------------------------------------+')\n",
        "  print('| +-------+                                                  |')\n",
        "  print('| |  ! !  | N-Gram Bot @ngrambot - 12h                       |')\n",
        "  print('| | [O_O] |                                                  |')\n",
        "  print('| |  | |  |                                                  |')\n",
        "  print('| +-------+                                                  |')\n",
        "  print('|                                                            |')\n",
        "\n",
        "  for line in tweet_to_lines(tweet):\n",
        "    padding = ' ' * (59 - len(line))\n",
        "    print('| ' + line + padding + '|')\n",
        "\n",
        "  print('|                                                            |')\n",
        "  print('| <3 31.4k                                                   |')\n",
        "  print('+------------------------------------------------------------+')\n",
        "\n",
        "def tweet_to_lines(tweet):\n",
        "  lines = []\n",
        "  curr_line = ''\n",
        "  for token in tweet.split():\n",
        "    if len(curr_line) + len(token) + 1 <= 59:\n",
        "      curr_line += token + ' '\n",
        "    else:\n",
        "      lines.append(curr_line)\n",
        "      curr_line = token + ' '\n",
        "  lines.append(curr_line)\n",
        "  return lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv4tSVyO9fL8",
        "outputId": "133bc327-151c-4da8-e9c0-3480e3982358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| pause 🤨 ✋ ⏸ ️                                              |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| nfts are a stupid, weird? bitch, a white person b like.    |\n",
            "| just an absurd number of drinks and now i'm 180 lbs. my    |\n",
            "| din-din is ready! oh me! they are goen straight to the     |\n",
            "| band txt                                                   |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| ratio                                                      |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| just found my childhood best friend from high school       |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| its time for a $ 5 if you'd like to call each other every  |\n",
            "| single character as my divine light is severed by god      |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| i'm glad i sold her crack!!                                |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| eyes rolling up into the euthanasia trapeze. the once-big  |\n",
            "| irony boys dm me any of these freaks fyi                   |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| i must be stopped.                                         |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| i think my neighbor                                        |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n",
            "+------------------------------------------------------------+\n",
            "| +-------+                                                  |\n",
            "| |  ! !  | N-Gram Bot @ngrambot - 12h                       |\n",
            "| | [O_O] |                                                  |\n",
            "| |  | |  |                                                  |\n",
            "| +-------+                                                  |\n",
            "|                                                            |\n",
            "| if i was like your camo. are we still don't have to give   |\n",
            "| you full speed ramming a woman for a 2km journey 😂         |\n",
            "|                                                            |\n",
            "| <3 31.4k                                                   |\n",
            "+------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  print_tweet(model.random_tweet())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "N-Gram Language Model on Tweets",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
